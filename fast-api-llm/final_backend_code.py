# -*- coding: utf-8 -*-
"""Final backend code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qcHJzBgeR_IvMPuPXCB_oSYu3-cJvhCe
"""

# !mkdir -p 'data/yash'
# https://arxiv.org/pdf/1706.03762.pdf
# https://arxiv.org/pdf/2307.09288.pdf
#!curl 'https://arxiv.org/pdf/2307.09288.pdf' -o 'data/llama2.pdf'

# !curl 'https://arxiv.org/pdf/2307.09288.pdf' -o 'data/yash/llama.pdf'

# Commented out IPython magic to ensure Python compatibility.
# %pip install llama-index llama-index-llms-huggingface llama-index-embeddings-huggingface llama-index-packs-subdoc-summary

from llama_index.packs.subdoc_summary import SubDocSummaryPack
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.huggingface import HuggingFaceInferenceAPI
from llama_index.core import SimpleDirectoryReader


llmAPi = HuggingFaceInferenceAPI(
    model_name="mistralai/Mistral-7B-Instruct-v0.2", token=HF_TOKEN
)

embed_model_HGGF = HuggingFaceEmbedding(
    model_name="BAAI/bge-small-en-v1.5"
)

documents = SimpleDirectoryReader("data/yash").load_data()

len(documents)


len(documents)

subdoc_summary_pack_mis = SubDocSummaryPack(
    documents,
    verbose=True,
    parent_chunk_size=8192,  # default,
    child_chunk_size=512,  # default
    llm=llmAPi,
    embed_model=embed_model_HGGF,
)

SubDocSummaryPack

from IPython.display import Markdown, display
from llama_index.core.response.notebook_utils import display_source_node

response = subdoc_summary_pack_mis.run("what are encoders and decoders?")
display(Markdown(str(response)))